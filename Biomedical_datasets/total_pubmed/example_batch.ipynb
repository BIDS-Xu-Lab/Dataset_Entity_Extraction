{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gy237/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import string\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pmid', 'title', 'abstract', 'journal', 'pubdate', 'authors',\n",
      "       'mesh_terms', 'pub_year', 'pub_month'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 加载.joblib文件\n",
    "old_data = joblib.load('/home/gy237/project/Biomedical_datasets/total_pubmed/abstracts_1-1252.joblib')\n",
    "print(old_data.columns)\n",
    "# ['pmid', 'title', 'abstract', 'journal', 'pubdate', 'authors','mesh_terms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pmid', 'title', 'abstract', 'journal', 'authors', 'mesh_terms',\n",
      "       'pub_year', 'pub_month'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "new_data = joblib.load('/home/gy237/project/Biomedical_datasets/total_pubmed/abstracts_1253-1575.joblib')\n",
    "print(new_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去重前： 39390916\n",
      "去重后： 37814430\n",
      "Index(['pmid', 'title', 'abstract', 'journal', 'pubdate', 'authors',\n",
      "       'mesh_terms', 'pub_year', 'pub_month'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.concat([old_data, new_data], axis=0)\n",
    "print('去重前：', len(combined_df))\n",
    "data = combined_df.drop_duplicates(subset=['pmid'], keep='last')\n",
    "print('去重后：', len(data))\n",
    "print(data.columns)\n",
    "# 去重前： 39390916\n",
    "# 去重后： 37814430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将abstracts分句并进行保存\n",
    "def ends_with_punctuation(text):\n",
    "    return text[-1] in string.punctuation if text else False\n",
    "\n",
    "def split_abstracts(abstract):\n",
    "    doc = nlp(abstract)\n",
    "    abstracts = [sent.text for sent in doc.sents]\n",
    "    assert len(abstracts) != 0, abstract\n",
    "        \n",
    "    new_abstracts = []\n",
    "    for j in abstracts:\n",
    "        j = j.strip().split('\\n')   # 有些内部会有\\n\n",
    "        for k in j:\n",
    "            if len(k.split(' ')) > 5 and ends_with_punctuation(k):\n",
    "                new_abstracts.append(k)\n",
    "    return new_abstracts\n",
    "\n",
    "def save_abstracts(data, name):\n",
    "    for index, row in data.iterrows():\n",
    "        pmid = row['pmid']\n",
    "        abstract = row['abstract']\n",
    "        doc = split_abstracts(abstract)\n",
    "        \n",
    "        # 以pmid为文件名创建txt文件\n",
    "        os.makedirs(f\"/home/gy237/project/Biomedical_datasets/total_pubmed/{name}\", exist_ok=True)\n",
    "        with open(f\"/home/gy237/project/Biomedical_datasets/total_pubmed/{name}/{pmid}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            for j in doc:\n",
    "                file.write(j + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, question):\n",
    "    # return 'Y'    # use, when you check the diagnoses list\n",
    "    client = OpenAI(api_key=\"sk-svcacct-Pqi-bFBAdqeGHBZO56dX8kbGpYm05g3dV920DDxOc7LNGpts6jpeYvRSwRDaSF15xT3BlbkFJd5kbvd-ja5Jh7jXMv6_OpXZKviW84lRFT3DGkPGgz43himYNc_7VzYwMWDhcn9FAA\")\n",
    "        \n",
    "    chat_return = client.chat.completions.create(model='gpt-4o-mini',temperature=0.0, messages=[{\"role\": \"system\", \"content\": prompt}, {\"role\":\"user\", \"content\": question}])\n",
    "\n",
    "    result = chat_return.choices[0].message.content\n",
    "    return result\n",
    "\n",
    "def process_chunk(index_list, abstract_list, prompt_list):\n",
    "    result = []\n",
    "    for i in trange(len(index_list)):\n",
    "        flag = True\n",
    "        count = 0\n",
    "        # while flag:\n",
    "        yn = generate(prompt_list[i], abstract_list[i])\n",
    "            # count += 1\n",
    "            # if yn in ['Y', 'N']:\n",
    "            #     flag = False\n",
    "            # elif count > 2:\n",
    "            #     print(f'Error, {abstract_list[i]}')\n",
    "            #     flag = False\n",
    "        result.append({'id': index_list[i], 'yn': yn})\n",
    "    return result\n",
    "\n",
    "def filter(prompt, batch, num_tasks):\n",
    "    index_list = batch.index.tolist()\n",
    "    abstracts = batch['abstract'].tolist()\n",
    "    prompt_list = [prompt]*len(index_list)\n",
    "    \n",
    "    index_list = np.array_split(index_list, num_tasks)\n",
    "    abstracts_list = np.array_split(abstracts, num_tasks)\n",
    "    prompt_list = np.array_split(prompt_list, num_tasks)\n",
    "\n",
    "    with Pool(num_tasks) as pool:\n",
    "        results = pool.starmap(process_chunk, zip(index_list, abstracts_list, prompt_list))\n",
    "    \n",
    "    for result in results:\n",
    "        for i in result:\n",
    "            batch.loc[i['id'], 'yn'] = i['yn']\n",
    "        \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''I will provide you with the abstract of an article. Your task is to determine if it contains any mentions of Datasets, Repositories, or Knowledge Bases. If any of these are mentioned, respond with Y and the mentions; otherwise, respond with N. Please note that you should lean towards outputting Y, as manual verification will be conducted later.\n",
    "Below are the definitions and examples of Datasets, Repositories, and Knowledge Bases for your reference:\n",
    "Dataset: A collection of data.\n",
    "Repository: A data hosting site that collects, manages, and stores datasets for secondary use in research.\n",
    "Knowledge Base: A collection of data or information about a particular subject. A knowledge base is typically curated.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''**Task:**\n",
    "You will be provided with the abstract of an article. Your goal is to determine whether it mentions any Datasets, Repositories, or Knowledge Bases. Importantly, each mention must include a specific name (e.g., a named dataset, repository, or knowledge base).\n",
    "**Output Format:**\n",
    "If any of these are mentioned, respond with: Y\n",
    "If none are mentioned, respond with: N\n",
    "**Guidelines:**\n",
    "Err on the side of caution and lean towards responding with \"Y\" when in doubt, as manual verification will follow.\n",
    "**Definitions for Reference:**\n",
    "Dataset: A structured collection of data. Surveys, interviews and questionnaires can be considered as dataset.\n",
    "Dataset Examples: \"2020-2021 Minimum Data Set 3.0\", \"Medicare datasets\", \"the Current Population Survey (CPS)\", \"the Kansas City Cardiomyopathy Questionnaire (KCCQ)\"\n",
    "Repository: A platform or site that collects, manages, and stores datasets for secondary use in research.\n",
    "Repository Examples: \"GenBank\", \"the Protein Data Bank (PDB)\", \"Scopus\", \"the Human Genome Project data\", \"the ReNDiS database\", \"the Canadian Cancer Registry\", \"International clinical guidelines\"\n",
    "Knowledge Base: A curated collection of information or data about a specific topic.\n",
    "Knowledge Base Examples: \"LinkedOmicsKB\", \"the Clinical Trial Knowledge Base\", \"mirtronDB\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:08<00:00,  2.59it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.46it/s]\n",
      "100%|██████████| 22/22 [00:08<00:00,  2.51it/s]\n",
      "100%|██████████| 22/22 [00:09<00:00,  2.40it/s]\n",
      "100%|██████████| 22/22 [00:09<00:00,  2.37it/s]\n",
      "100%|██████████| 22/22 [00:09<00:00,  2.33it/s]\n",
      "100%|██████████| 22/22 [00:10<00:00,  2.20it/s]\n",
      "100%|██████████| 22/22 [00:10<00:00,  2.17it/s]\n",
      "100%|██████████| 22/22 [00:10<00:00,  2.13it/s]\n",
      "100%|██████████| 22/22 [00:12<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/tmp.hFA2tbdmXU/ipykernel_2181053/1847318057.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch.loc[i['id'], 'yn'] = i['yn']\n"
     ]
    }
   ],
   "source": [
    "batch = data.sample(n=300, random_state=42)\n",
    "print(len(batch))\n",
    "df_filtered = batch[batch['abstract'].apply(lambda x: x.strip()) != '']\n",
    "print(len(df_filtered))\n",
    "\n",
    "df_filtered = filter(prompt, df_filtered, 10)\n",
    "df_filtered.to_json('/home/gy237/project/Biomedical_datasets/total_pubmed/df_filtered.json', orient='records', lines=True, indent=4)\n",
    "\n",
    "df_filtered = df_filtered[df_filtered['yn']!='N']\n",
    "print(len(df_filtered))\n",
    "\n",
    "# batch = df_filtered.sample(n=200, random_state=42)\n",
    "# save_abstracts(batch, 'total_PubMed_sample_to_decide_key_words/Batch_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_repositories = [\n",
    "        \" data\", \"dataset\", \"database\", \"repository\", \"survey\", \"questionnaire\", \"PubMed\",\" NCBI \",\"Scopus\",\"Kaggle\",\"knowledge base\",\n",
    "        \"Mendeley\",\"Education Resources Information Center\",\"Science Open\",\"Web of Science\",\"Cochrane Library\",\"EMbase\",\n",
    "        \"Chinese Biomedical Literature Database\",\"Medline\",\n",
    "        \"Kaggle\", \"Google Dataset Search\", \"Zenodo\", \"Dryad\", \"Figshare\",\n",
    "        \"Open Data Portal\", \"World Bank Open Data\", \"UN Data\", \"Data.gov\", \n",
    "        \"DataHub\", \"IEEE DataPort\", \"Mendeley Data\", \n",
    "        \"Open Science Framework\", \"AWS Open Data Registry\", \"Harvard Dataverse\",\n",
    "        \"ICPSR\",\"re3data.org\", \"PLOS Data Repository\", \"UK Data Service\", \n",
    "        \"Humanitarian Data Exchange\", \"UCI Machine Learning Repository\", \n",
    "        \"Statista\", \"Quandl\", \"OpenStreetMap\", \"EarthData\", \"Global Health Observatory\", \n",
    "        \"GBIF\", \"GCMD\", \"Eurostat\", \"OECD Data\", \n",
    "        \"Climate Data Store\", \"FAOSTAT\", \"Geonames\", \"NCEI\", \n",
    "        \"Copernicus Open Access Hub\", \"Landsat Data Repository\", \n",
    "        \"National Cancer Institute Genomic Data Commons\", \n",
    "        \"Census Bureau Data\", \"IMF Data\", \"OpenAIRE\", \n",
    "        \"NCES\", \"PERSEE\", \"StatBank Denmark\", \n",
    "        \"Australian Data Archive\", \"China Statistical Yearbook\", \n",
    "        \"Open Knowledge Foundation CKAN\", \"BigQuery Public Datasets\", \n",
    "        \"UNESCO Institute for Statistics Data Centre\", \"Linked Data Platform\"\n",
    "]\n",
    "\n",
    "life_science_repository = [\n",
    "        \"ClinicalTrials\", \"clinical trials\",\n",
    "        \"GenBank\",\"Gene Expression Omnibus\",\"PubChem\",\"Protein Data Bank\",\"UniProt\",\n",
    "        \"Genotype-Tissue Expression\",\"Bioproject\",\"dbSNP\",\"ClinVar\",\"PhysioNet\",\"National Alzheimer's Coordinating Center\",\n",
    "        \"Sequence Read Archive\",\"LINCS\",\"ImmPort\",\"dbGaP\",\"The Cancer Imaging Archive\",\"CellChat\",\n",
    "        \"FlyBase\",\"BioPortal\",\"Mouse Genome Informatics\",\"National COVID Cohort Collaborative\",\"Saccharomyces Genome Database\",\n",
    "        \"Genomic Data Commons\",\"PeptideAtlas\",\"WormBase\",\"International Mouse Phenotyping Consortium\",\n",
    "        \"BindingDB\",\"CHILDES\",\"NITRC\",\"Rat Genome Database\",\"Immunological Genome Project\",\n",
    "        \"Investigational New Drug Applications\",\"ICPSR\",\"HOMD\",\"AphasiaBank\",\"OpenNeuro\",\n",
    "        \"Metabolomics\",\"4D-Nucleome\",\"NDEx\",\"Mouse Phenome Database\",\"BioLINCC\",\n",
    "        \"National Sleep Research Resource\",\"Xenbase\",\n",
    "        \"NIH Genetic Testing Registry\",\"BMRB\",\"Kids First Data Resource\",\n",
    "        \"Monarch Initiative\",\"dbVar\",\"ZFIN\",\n",
    "]\n",
    "total_target = general_repositories + life_science_repository\n",
    "print(len(total_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个空的DataFrame用于存储匹配结果\n",
    "filtered_data = pd.DataFrame()\n",
    "key_dic = {}\n",
    "\n",
    "# 将数据切割成n个部分\n",
    "def process_chunk(chunk, total_target):\n",
    "    chunk_filtered_data = pd.DataFrame()\n",
    "    chunk_key_dic = {}\n",
    "\n",
    "    for keyword in total_target:\n",
    "        matched_rows = chunk[chunk['abstract'].str.contains(keyword, case=False, na=False)]\n",
    "        chunk_key_dic[keyword] = len(matched_rows)\n",
    "        chunk_filtered_data = pd.concat([chunk_filtered_data, matched_rows])\n",
    "\n",
    "    # 去重\n",
    "    chunk_filtered_data = chunk_filtered_data.drop_duplicates(subset=['pmid'], keep='last')\n",
    "\n",
    "    return chunk_filtered_data, chunk_key_dic\n",
    "\n",
    "def parallel_process(data, total_target, num_chunks=20):\n",
    "    print(len(data))\n",
    "    # 将数据切割成多个块\n",
    "    chunks = np.array_split(data, num_chunks)\n",
    "    \n",
    "    # 使用多进程处理每个块\n",
    "    with multiprocessing.Pool(processes=num_chunks) as pool:\n",
    "        results = pool.starmap(process_chunk, [(chunk, total_target) for chunk in chunks])\n",
    "\n",
    "    # 合并结果\n",
    "    final_filtered_data = pd.DataFrame()\n",
    "    final_key_dic = {}\n",
    "\n",
    "    for chunk_filtered_data, chunk_key_dic in results:\n",
    "        final_filtered_data = pd.concat([final_filtered_data, chunk_filtered_data])\n",
    "        for key, value in chunk_key_dic.items():\n",
    "            if key in final_key_dic:\n",
    "                final_key_dic[key] += value\n",
    "            else:\n",
    "                final_key_dic[key] = value\n",
    "\n",
    "    # 去重\n",
    "    final_filtered_data = final_filtered_data.drop_duplicates(subset=['pmid'], keep='last')\n",
    "\n",
    "    return final_filtered_data, final_key_dic\n",
    "\n",
    "# 假设data和total_target已经准备好\n",
    "filtered_data, key_dic = parallel_process(data, total_target, num_chunks=10)\n",
    "\n",
    "# 打印结果\n",
    "print(len(filtered_data))\n",
    "print(key_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(filtered_data, '/home/gy237/project/Biomedical_datasets/total_pubmed/abstracts_filtere_1-1575.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/gy237/project/Biomedical_datasets/total_pubmed/abstracts_filtere_1-1575.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(key_dic, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10501984\n"
     ]
    }
   ],
   "source": [
    "filtered_df = joblib.load('/home/gy237/project/Biomedical_datasets/total_pubmed/abstracts_filtere_1-1575.joblib')\n",
    "print(len(filtered_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list = [\n",
    "    '/home/gy237/project/Biomedical_datasets/total_pubmed/Batch_1/agreement',\n",
    "    '/home/gy237/project/Biomedical_datasets/total_pubmed/Batch_2/Batch_2',\n",
    "    '/home/gy237/project/Biomedical_datasets/total_pubmed/Batch_3',\n",
    "]\n",
    "\n",
    "folder_list = [\n",
    "    \"/home/gy237/project/Biomedical_datasets/total_pubmed/Gui_Batches\",\n",
    "    \"/home/gy237/project/Biomedical_datasets/total_pubmed/Kalpana_Batches\"\n",
    "]\n",
    "\n",
    "for i in folder_list:\n",
    "    f_list.extend([f'{i}/{j}' for j in os.listdir(i)])\n",
    "print(len(f_list))\n",
    "\n",
    "exist_pmids = []\n",
    "for i in f_list:\n",
    "    exist_pmids.extend(os.listdir(i))\n",
    "exist_pmids = [i.split('.')[0] for i in exist_pmids]\n",
    "print('filtered_df:', len(filtered_df))\n",
    "print('exist_pmids:', len(set(exist_pmids)))\n",
    "\n",
    "\n",
    "filtered_df_left = filtered_df[~filtered_df['pmid'].isin(exist_pmids)]\n",
    "print('filtered_df_left :', len(filtered_df_left))\n",
    "\n",
    "\n",
    "batch = filtered_df_left.sample(n=100, random_state=42)\n",
    "save_abstracts(batch, 'Kalpana_Batches/Batch_10')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
