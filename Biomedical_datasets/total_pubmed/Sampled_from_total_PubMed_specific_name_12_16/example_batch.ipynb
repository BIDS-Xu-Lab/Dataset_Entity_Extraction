{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import string\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26406687\n",
      "Index(['pmid', 'title', 'abstract', 'journal', 'pubdate', 'authors',\n",
      "       'mesh_terms', 'pub_year', 'pub_month'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_26406687 = joblib.load('/home/gy237/project/Biomedical_datasets/total_pubmed/Sampled_from_total_PubMed_specific_name_12_16/abstracts_nonull_1-1575.joblib')\n",
    "print(len(data_26406687))\n",
    "print(data_26406687.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将abstracts分句并进行保存\n",
    "def ends_with_punctuation(text):\n",
    "    return text[-1] in string.punctuation if text else False\n",
    "\n",
    "def split_abstracts(abstract):\n",
    "    doc = nlp(abstract)\n",
    "    abstracts = [sent.text for sent in doc.sents]\n",
    "    assert len(abstracts) != 0, abstract\n",
    "        \n",
    "    new_abstracts = []\n",
    "    for j in abstracts:\n",
    "        j = j.strip().split('\\n')   # 有些内部会有\\n\n",
    "        for k in j:\n",
    "            if len(k.split(' ')) > 5 and ends_with_punctuation(k):\n",
    "                new_abstracts.append(k)\n",
    "    return new_abstracts\n",
    "\n",
    "def save_abstracts(data, folder):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    for index, row in data.iterrows():\n",
    "        pmid = row['pmid']\n",
    "        abstract = row['abstract']\n",
    "        yn = row['yn']\n",
    "        doc = split_abstracts(abstract)\n",
    "        \n",
    "        # 以pmid为文件名创建txt文件\n",
    "        os.makedirs(f\"{folder}\", exist_ok=True)\n",
    "        with open(f\"{folder}/{pmid}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            for j in doc:\n",
    "                file.write(j + '\\n')\n",
    "            file.write(f'{yn} (No need to annotate, just for notice).\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, question):\n",
    "    # return 'Y'    # use, when you check the diagnoses list\n",
    "    client = OpenAI()\n",
    "        \n",
    "    chat_return = client.chat.completions.create(model='gpt-4o-mini',temperature=0.0, messages=[{\"role\": \"system\", \"content\": prompt}, {\"role\":\"user\", \"content\": question}])\n",
    "\n",
    "    result = chat_return.choices[0].message.content\n",
    "    return result\n",
    "\n",
    "def process_chunk(index_list, abstract_list, prompt_list):\n",
    "    result = []\n",
    "    for i in trange(len(index_list)):\n",
    "        flag = True\n",
    "        count = 0\n",
    "        # while flag:\n",
    "        yn = generate(prompt_list[i], abstract_list[i])\n",
    "            # count += 1\n",
    "            # if yn in ['Y', 'N']:\n",
    "            #     flag = False\n",
    "            # elif count > 2:\n",
    "            #     print(f'Error, {abstract_list[i]}')\n",
    "            #     flag = False\n",
    "        result.append({'id': index_list[i], 'yn': yn})\n",
    "    return result\n",
    "\n",
    "def filter(prompt, batch, num_tasks):\n",
    "    index_list = batch.index.tolist()\n",
    "    abstracts = batch['abstract'].tolist()\n",
    "    prompt_list = [prompt]*len(index_list)\n",
    "    \n",
    "    index_list = np.array_split(index_list, num_tasks)\n",
    "    abstracts_list = np.array_split(abstracts, num_tasks)\n",
    "    prompt_list = np.array_split(prompt_list, num_tasks)\n",
    "\n",
    "    with Pool(num_tasks) as pool:\n",
    "        results = pool.starmap(process_chunk, zip(index_list, abstracts_list, prompt_list))\n",
    "    \n",
    "    for result in results:\n",
    "        for i in result:\n",
    "            batch.loc[i['id'], 'yn'] = i['yn']\n",
    "        \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''**Task:**\n",
    "You will be provided with the abstract of an article. Your goal is to determine whether it mentions any Datasets, Repositories, or Knowledge Bases. Importantly, each mention must include a specific name.\n",
    "**Output Format:**\n",
    "If any names of these are mentioned, respond with: Y and the names\n",
    "If none names are mentioned, respond with: N\n",
    "**Guidelines:**\n",
    "1. When determining your response, respond with “Y” when any named entity that includes, provides or refers to data is mentioned. \n",
    "2. Please note that you should also respond \"Y\" even if you don't recognize the name.\n",
    "3. If you encounter any hesitation or need to think, you should respond with “Y”.\n",
    "**Definitions for Reference:**\n",
    "Specific name: A name that allows the mentions to be identified, usually containing capital letters.\n",
    "Examples: 'yeast dataset' and 'a dataset of 150 COVID-19 RCT abstracts' are not specific names.\n",
    "Dataset: A structured collection of data. Note that any named surveys, interviews and questionnaires can be considered as Dataset when they have specific names.\n",
    "Dataset Examples: \"2020-2021 Minimum Data Set 3.0\", \"Medicare datasets\", \"2001 Participation and Activity Limitation Survey (PALS)\", \"the Kansas City Cardiomyopathy Questionnaire (KCCQ)\"\n",
    "Repository: A platform or site that collects, manages, and stores datasets for secondary use in research. Note that search platforms can be considered as Repository when they have specific names.\n",
    "Repository Examples: \"GenBank\", \"ClinicalTrials\", \"the Protein Data Bank (PDB)\", \"PubMed\", \"the Human Genome Project data\", \"the ReNDiS database\", \"the Canadian Cancer Registry\", \"International clinical guidelines\"\n",
    "Knowledge Base: A curated collection of information or data about a specific topic.\n",
    "Knowledge Base Examples: \"LinkedOmicsKB\", \"the Clinical Trial Knowledge Base\", \"mirtronDB\"\n",
    "**Abstract:**\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载之前的200个annotated pmid and abstracts\n",
    "batch1 = '/home/gy237/project/Biomedical_datasets/total_pubmed/Batch_1/agreement'\n",
    "batch2 = '/home/gy237/project//Biomedical_datasets/total_pubmed/Batch_2/Batch_2'\n",
    "true = '/home/gy237/project/Biomedical_datasets/total_pubmed/Sampled_from_total_PubMed_specific_name_12_16/specific_name_pmid.txt'\n",
    "\n",
    "with open(true, 'r') as file:\n",
    "    true_pmid = file.readlines()\n",
    "true_pmid = [i.strip() for i in true_pmid]\n",
    "\n",
    "pmid1 = os.listdir(batch1)\n",
    "pmid2 = os.listdir(batch2)\n",
    "\n",
    "pmids = [i.split('.')[0] for i in pmid1] + [i.split('.')[0] for i in pmid2]\n",
    "assert set(true_pmid).issubset(pmids)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=[\"pmid\", \"abstract\"])\n",
    "for i in pmid1:\n",
    "    with open(f\"{batch1}/{i}\", 'r') as f:\n",
    "        text = f.readlines()\n",
    "        text = [j.strip() for j in text]\n",
    "        text = ' '.join(text)\n",
    "    i = i.split('.')[0]\n",
    "    df.loc[len(df), 'pmid'] = i\n",
    "    df.loc[len(df)-1, 'abstract'] = text\n",
    "\n",
    "for i in pmid2:\n",
    "    with open(f\"{batch2}/{i}\", 'r') as f:\n",
    "        text = f.readlines()\n",
    "        text = [j.strip() for j in text]\n",
    "        text = ' '.join(text)\n",
    "    i = i.split('.')[0]\n",
    "    df.loc[len(df), 'pmid'] = i\n",
    "    df.loc[len(df)-1, 'abstract'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:08<00:00,  2.49it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.49it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.39it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.36it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.27it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.21it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.14it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.95it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.82it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "27\n",
      "0\n",
      "Accuracy: 27/27\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "# 使用交互式API\n",
    "df_yn = filter(prompt, df, 10)\n",
    "\n",
    "yn_pmid = df_yn[df_yn['yn'] != 'N']['pmid'].tolist()\n",
    "print(len(yn_pmid))\n",
    "print(len(true_pmid))\n",
    "\n",
    "error = []\n",
    "for i in true_pmid:\n",
    "    if i not in yn_pmid:\n",
    "        j = f'{i}.txt'\n",
    "        error.append(j)\n",
    "print(len(error))\n",
    "print(f'Accuracy: {len(true_pmid)-len(error)}/{len(true_pmid)}')\n",
    "\n",
    "# gpt-4o-mini accuracy 16/27\n",
    "# gpt-4o accuracy 27/27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500\n",
      "Index(['pmid', 'title', 'abstract', 'journal', 'pubdate', 'authors',\n",
      "       'mesh_terms', 'pub_year', 'pub_month'],\n",
      "      dtype='object')\n",
      "5000\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "batch = data_26406687.sample(n=6500, random_state=42)\n",
    "batch = batch.reset_index(drop=True)\n",
    "print(len(batch))\n",
    "print(batch.columns)\n",
    "\n",
    "batch_1 = joblib.load('/home/gy237/project/Biomedical_datasets/total_pubmed/Sampled_from_total_PubMed_specific_name_12_16/Batch_1_sample_5000_12_16/Batch_1_sample_5000_12_16.joblib')\n",
    "print(len(batch_1))\n",
    "\n",
    "filtered_df = batch[~batch['pmid'].isin(batch_1['pmid'])]\n",
    "print(len(filtered_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:27<00:00,  2.73it/s]\n",
      "100%|██████████| 75/75 [00:27<00:00,  2.69it/s]\n",
      " 79%|███████▊  | 59/75 [00:28<00:07,  2.26it/s]\n",
      "100%|██████████| 75/75 [00:28<00:00,  2.59it/s]\n",
      "100%|██████████| 75/75 [00:29<00:00,  2.57it/s]\n",
      "100%|██████████| 75/75 [00:30<00:00,  2.49it/s]\n",
      "100%|██████████| 75/75 [00:30<00:00,  2.44it/s]\n",
      "100%|██████████| 75/75 [00:30<00:00,  2.43it/s]\n",
      "100%|██████████| 75/75 [00:31<00:00,  2.39it/s]\n",
      "100%|██████████| 75/75 [00:30<00:00,  2.43it/s]\n",
      "100%|██████████| 75/75 [00:32<00:00,  2.32it/s]\n",
      "100%|██████████| 75/75 [00:32<00:00,  2.29it/s]\n",
      "100%|██████████| 75/75 [00:32<00:00,  2.29it/s]\n",
      "100%|██████████| 75/75 [00:33<00:00,  2.25it/s]\n",
      "100%|██████████| 75/75 [00:33<00:00,  2.25it/s]\n",
      "100%|██████████| 75/75 [00:34<00:00,  2.18it/s]\n",
      "100%|██████████| 75/75 [00:34<00:00,  2.15it/s]\n",
      "100%|██████████| 75/75 [00:35<00:00,  2.14it/s]\n",
      "100%|██████████| 75/75 [00:36<00:00,  2.08it/s]\n",
      "100%|██████████| 75/75 [00:47<00:00,  1.59it/s]\n",
      "/tmp/tmp.trKQcapNiD/ipykernel_2256733/1847318057.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch.loc[i['id'], 'yn'] = i['yn']\n"
     ]
    }
   ],
   "source": [
    "batch_yn = filter(prompt, filtered_df, 20)\n",
    "print(len(batch_yn[batch_yn['yn'] != 'N']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "# 保存文件\n",
    "name = 'Batch_3_sample_6500_12_16'\n",
    "\n",
    "joblib.dump(batch_yn, f'/home/gy237/project/Biomedical_datasets/total_pubmed/Sampled_from_total_PubMed_specific_name_12_16/{name}/{name}.joblib')\n",
    "\n",
    "print(len(batch_yn))\n",
    "batch_y = batch_yn[batch_yn['yn'] != 'N']\n",
    "print(len(batch_y))\n",
    "\n",
    "# 指定第一组的数量\n",
    "group1_size = 56\n",
    "# 随机打乱索引\n",
    "shuffled_indices = np.random.permutation(batch_y.index)\n",
    "# 分成两组\n",
    "group1_indices = shuffled_indices[:group1_size]\n",
    "group2_indices = shuffled_indices[group1_size:]\n",
    "group1 = batch_y.loc[group1_indices]\n",
    "group2 = batch_y.loc[group2_indices]\n",
    "\n",
    "save_abstracts(group1, f'/home/gy237/project/Biomedical_datasets/total_pubmed/Sampled_from_total_PubMed_specific_name_12_16/{name}/{name}')\n",
    "save_abstracts(group2, f'/home/gy237/project/Biomedical_datasets/total_pubmed/Sampled_from_total_PubMed_specific_name_12_16/The_rest/{name}_number_{len(group2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = data.sample(n=5000, random_state=42)\n",
    "# batch = batch.reset_index(drop=True)\n",
    "# print(len(batch))\n",
    "# print(batch.columns)\n",
    "\n",
    "output_file = '/home/gy237/project/Biomedical_datasets/total_pubmed/Sampled_from_total_PubMed_specific_name_12_16/sample_5000_to_openai.jsonl'\n",
    "\n",
    "df = df.drop_duplicates(subset=['pmid'], keep='last')\n",
    "df = df.reset_index(drop=True)\n",
    "for i in range(len(df)):\n",
    "    dic = {\"custom_id\": df.loc[i, 'pmid'], \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"system\", \"content\": prompt},{\"role\": \"user\", \"content\": df.loc[i, 'abstract']}],\"max_tokens\": 16000}}\n",
    "    with open(output_file,'a', encoding='utf-8') as file:\n",
    "        file.write(json.dumps(dic) + '\\n')\n",
    "\n",
    "# for i in range(len(batch)):\n",
    "#     dic = {\"custom_id\": batch.loc[i, 'pmid'], \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"system\", \"content\": prompt},{\"role\": \"user\", \"content\": batch.loc[i, 'abstract']}],\"max_tokens\": 49000}}\n",
    "#     with open(output_file,'a', encoding='utf-8') as file:\n",
    "#         file.write(json.dumps(dic) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(id='batch_6760a2410a0081909b888c8d085a0fd4', completion_window='24h', created_at=1734386241, endpoint='/v1/chat/completions', input_file_id='file-JgfzLMAaS5ZVLvwQFs8Akg', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1734472641, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Filter out pmids with specific names'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"Filter out pmids with specific names\"\n",
    "client = OpenAI()\n",
    "# upload your batch input file\n",
    "batch_input_file = client.files.create(\n",
    "  file=open(f\"/home/gy237/project/Biomedical_datasets/total_pubmed/Sampled_from_total_PubMed_specific_name_12_16/sample_5000_to_openai.jsonl\", \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "\n",
    "# create the batch, only the description can change\n",
    "batch_input_file_id = batch_input_file.id\n",
    "client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": f'{name}'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_67609fd318208190b4b6c8d6523160be', completion_window='24h', created_at=1734385619, endpoint='/v1/chat/completions', input_file_id='file-G2MBddu6JjdmvCRcjcUnaR', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1734386639, error_file_id=None, errors=None, expired_at=None, expires_at=1734472019, failed_at=None, finalizing_at=1734386619, in_progress_at=1734385619, metadata={'description': 'Filter out pmids with specific names'}, output_file_id='file-4D9n2fxX9U64CgjRJGWzGB', request_counts=BatchRequestCounts(completed=184, failed=0, total=184))\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "batch_api = client.batches.retrieve(\"batch_67609fd318208190b4b6c8d6523160be\")\n",
    "# client.batches.cancel(\"batch_abc123\")\n",
    "print(batch_api)\n",
    "print(batch_api.status)\n",
    "\n",
    "Batch(id='batch_67609fd318208190b4b6c8d6523160be', completion_window='24h', created_at=1734385619, endpoint='/v1/chat/completions', input_file_id='file-G2MBddu6JjdmvCRcjcUnaR', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1734386639, error_file_id=None, errors=None, expired_at=None, expires_at=1734472019, failed_at=None, finalizing_at=1734386619, in_progress_at=1734385619, metadata={'description': 'Filter out pmids with specific names'}, output_file_id='file-4D9n2fxX9U64CgjRJGWzGB', request_counts=BatchRequestCounts(completed=184, failed=0, total=184))\n",
    "\n",
    "\n",
    "# curl https://api.openai.com/v1/files/file-4D9n2fxX9U64CgjRJGWzGB/content \\\n",
    "# -H \"Authorization: Bearer $OPENAI_API_KEY\" > /home/gy237/project/Biomedical_datasets/total_pubmed/Sampled_from_total_PubMed_specific_name_12_16/batch_output_2.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "{'id': 'batch_req_6760964fbd608190bbee0810f56c3e14', 'custom_id': '8523925', 'response': {'status_code': 200, 'request_id': '0c82e98f0f99937797fc449e5c620cad', 'body': {'id': 'chatcmpl-AfCZ7z5AS6bT9FDa84Mg7DEuJ85bl', 'object': 'chat.completion', 'created': 1734383113, 'model': 'gpt-4o-2024-08-06', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'N', 'refusal': None}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 685, 'completion_tokens': 1, 'total_tokens': 686, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'system_fingerprint': 'fp_9faba9f038'}}, 'error': None}\n",
      "32\n",
      "27\n",
      "1\n",
      "['19269992']\n",
      "Accuracy: 26/27\n",
      "Total cost: 0.17054249999999999\n"
     ]
    }
   ],
   "source": [
    "with open('/home/gy237/project/Biomedical_datasets/total_pubmed/Sampled_from_total_PubMed_specific_name_12_16/batch_output.jsonl', 'r', encoding='utf-8') as file:\n",
    "    data = []\n",
    "    for line in file:\n",
    "        data.append(json.loads(line.strip()))\n",
    "\n",
    "print(len(data))\n",
    "print(data[0])\n",
    "\n",
    "\n",
    "# merge 回去\n",
    "for i in data:\n",
    "    pmid = i['custom_id']\n",
    "    yn = i['response']['body']['choices'][0]['message']['content']\n",
    "\n",
    "    # 计算cost\n",
    "    prompt_tokens = i['response']['body']['usage']['prompt_tokens']\n",
    "    completion_tokens = i['response']['body']['usage']['completion_tokens']\n",
    "    cost = (prompt_tokens*1.25 + completion_tokens*5)/1000000\n",
    "\n",
    "    df.loc[df['pmid'] == pmid, 'yn'] = yn\n",
    "    df.loc[df['pmid'] == pmid, 'cost'] = cost\n",
    "\n",
    "# for i in data:\n",
    "#     pmid = i['custom_id']\n",
    "#     yn = i['response']['body']['choices'][0]['message']['content']\n",
    "#     batch.loc[batch['pmid'] == pmid, 'yn'] = yn\n",
    "\n",
    "\n",
    "yn_pmid = df[df['yn'] != 'N']['pmid'].tolist()\n",
    "print(len(yn_pmid))\n",
    "print(len(true_pmid))\n",
    "\n",
    "error = []\n",
    "for i in true_pmid:\n",
    "    if i not in yn_pmid:\n",
    "        error.append(i)\n",
    "print(len(error))\n",
    "print(error)\n",
    "print(f'Accuracy: {len(true_pmid)-len(error)}/{len(true_pmid)}')\n",
    "\n",
    "total_cost = df['cost'].sum()\n",
    "print(\"Total cost:\", total_cost)\n",
    "# 184 abstracts cost 0.1705$ using Batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(filtered_data, '/home/gy237/project/Biomedical_datasets/total_pubmed/abstracts_filtere_1-1575.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件的 SHA-256 哈希值: 4218dd64a7f04a96e0d25a1f1232b7b904af1936c2b963f573d6da3dade4f8ea\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def calculate_file_hash(file_path, algorithm='sha256', buffer_size=65536):\n",
    "    \"\"\"\n",
    "    计算文件的哈希值。\n",
    "    \n",
    "    参数:\n",
    "        file_path (str): 文件路径。\n",
    "        algorithm (str): 哈希算法 ('md5', 'sha1', 'sha256'等)。\n",
    "        buffer_size (int): 每次读取的字节数，默认为 64KB。\n",
    "    \n",
    "    返回:\n",
    "        str: 文件的哈希值。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 根据算法名称创建哈希对象\n",
    "        hash_func = hashlib.new(algorithm)\n",
    "        \n",
    "        # 按块读取文件并更新哈希\n",
    "        with open(file_path, 'rb') as f:\n",
    "            while chunk := f.read(buffer_size):\n",
    "                hash_func.update(chunk)\n",
    "        \n",
    "        # 返回十六进制哈希值\n",
    "        return hash_func.hexdigest()\n",
    "    except Exception as e:\n",
    "        print(f\"计算哈希值时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "# 示例用法\n",
    "file_path = \"Batch_1_sample_5000_12_16/Batch_1_sample_5000_12_16.joblib\"\n",
    "hash_value = calculate_file_hash(file_path, algorithm='sha256')\n",
    "if hash_value:\n",
    "    print(f\"文件的 SHA-256 哈希值: {hash_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "Index(['pmid', 'title', 'abstract', 'journal', 'pubdate', 'authors',\n",
      "       'mesh_terms', 'pub_year', 'pub_month', 'yn'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "data_26406687 = joblib.load('/home/gy237/project/Biomedical_datasets/total_pubmed/Sampled_from_total_PubMed_specific_name_12_16/Batch_1_sample_5000_12_16.joblib')\n",
    "print(len(data_26406687))\n",
    "print(data_26406687.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
